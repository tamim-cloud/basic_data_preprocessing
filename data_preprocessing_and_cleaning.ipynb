{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8m/NTIRs+R6uDisfIT8Q0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamim-cloud/basic_data_preprocessing/blob/main/data_preprocessing_and_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xb6RW_rXxc6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSsAj2P6ZYCX",
        "outputId": "d2b0398a-c664-46eb-9a58-35e4ffea7ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/archive.zip, /content/archive.zip.zip or /content/archive.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path= \"/content/IMDB Dataset.csv\""
      ],
      "metadata": {
        "id": "SiOriV1maYER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "YrX5ilE0Z48j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "954fd47c-957f-40e4-eac8-4f8d4408b6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/IMDB Dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3425852144.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/IMDB Dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "iEf3ugxtaGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "pKabrn91aHWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.head(100)"
      ],
      "metadata": {
        "id": "RWDI_HVUa1k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "sxe5FmTnbByp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"review\"][0]"
      ],
      "metadata": {
        "id": "O3s9EO4JbDOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Lower Casing**"
      ],
      "metadata": {
        "id": "6K3pBR8mdIvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"review\"]=df[\"review\"].str.lower()"
      ],
      "metadata": {
        "id": "4BWe5naIcLen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"review\"][0]"
      ],
      "metadata": {
        "id": "DYIOPkprcVI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. HTML tag removing**"
      ],
      "metadata": {
        "id": "B_3cZ3ckdV82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.<br /><br />the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word.<br /><br />it is called oz as that is the nickname given to the oswald maximum security state penitentary. it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. em city is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />i would say the main appeal of the show is due to the \""
      ],
      "metadata": {
        "id": "tZ43vafGcXzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_html_tag(text):\n",
        "  pattern=re.compile('<.*?>')\n",
        "  return pattern.sub(r'',text)"
      ],
      "metadata": {
        "id": "0pEUq8iSeswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_html_tag(text)"
      ],
      "metadata": {
        "id": "C3Udkww3e1Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"review\"]=df[\"review\"].apply(remove_html_tag)"
      ],
      "metadata": {
        "id": "EWzLbFtJffPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"review\"][1]"
      ],
      "metadata": {
        "id": "M7TJpTSTgEjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. URL removing**"
      ],
      "metadata": {
        "id": "NSklwkS-deu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def url_removing(text):\n",
        "  pattern=re.compile(r'https?://S+|www\\.\\S+')\n",
        "  return pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "9jTDhIj3gGbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1=\"this is my colab today practice link https://colab.research.google.com/drive/1bjYLer7k-tLeCfop52gTouM8sezwaX2N#scrollTo=xb4BFgEAeRQt\""
      ],
      "metadata": {
        "id": "xb4BFgEAeRQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=url_removing(text1)"
      ],
      "metadata": {
        "id": "kwVUf9Pree0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "rDR9avE-gqVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2=\"this is youtube video link which one i am watching now https://www.youtube.com/watch?v=oh2Zml3bWoI , you can try this\""
      ],
      "metadata": {
        "id": "T3OwxSsyeiMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_removing(text2)"
      ],
      "metadata": {
        "id": "iwb1xq-1glJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Punctuation Handaling**"
      ],
      "metadata": {
        "id": "Gxjs9UzXhR_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string,time"
      ],
      "metadata": {
        "id": "Af-fq4A8hGcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "id": "39uAISjwzQOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude=string.punctuation"
      ],
      "metadata": {
        "id": "ztpuOfjfzgWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_text=\"this is tamim, and i am from varendra university. today i am practice data cleaning and data preprocesing?\""
      ],
      "metadata": {
        "id": "kEBR-5dazsb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(punctuation_text):\n",
        "  for char in exclude:\n",
        "    punctuation_text=punctuation_text.replace(char,'')\n",
        "  return punctuation_text"
      ],
      "metadata": {
        "id": "yAGLGWsSzTk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punctuation(punctuation_text)"
      ],
      "metadata": {
        "id": "Bwem43kJ0H-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start=time.time()\n",
        "remove_punctuation(punctuation_text)\n",
        "time1=time.time()-start\n",
        "print(time1*50000)"
      ],
      "metadata": {
        "id": "OdbaZLHX5n27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#another process\n",
        "def remove_punctuation1(text):\n",
        "  return text.translate(str.maketrans('','',exclude))"
      ],
      "metadata": {
        "id": "UYMjqWBB0NXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punctuation1(punctuation_text)"
      ],
      "metadata": {
        "id": "D7xLCM152XY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time complexity\n",
        "start=time.time()\n",
        "remove_punctuation1(punctuation_text)\n",
        "time2=time.time()-start\n",
        "print(time2*50000)"
      ],
      "metadata": {
        "id": "yXZqIuyB5EFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Conversation Handaling**"
      ],
      "metadata": {
        "id": "iGBEbmcR7KXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=\"this is tamim, i am from varendra university, and today i am practicing nlp for GENAI\""
      ],
      "metadata": {
        "id": "heot3Oge5auP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "MKmyz3BkWYHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "dDQ13cyqWryq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=word_tokenize(data)"
      ],
      "metadata": {
        "id": "tTrXUu3AWiGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "id": "uek71a92WnK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Conversion Handle**"
      ],
      "metadata": {
        "id": "RoBFI_d-ivxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_text={\n",
        "    'ASAP': 'AS SOON AS POSSIBLE',\n",
        "    'AFAIK': 'AS FOR AS I KNOW'\n",
        "}\n",
        "def chat_conversation(text):\n",
        "  new_text=[]\n",
        "  for w in text.split():\n",
        "    if w.upper() in chat_text:\n",
        "      new_text.append(chat_text[w.upper()])\n",
        "    else:\n",
        "      new_text.append(w)\n",
        "  return \" \".join(new_text)\n",
        "\n",
        "chat_conversation(\"do this work asap\")"
      ],
      "metadata": {
        "id": "ho-E9mp2Wwn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorrecct text Handaling\n"
      ],
      "metadata": {
        "id": "HTzADDQ3uOS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_text=\"Onc upon a tim, in a smal villag, their livved a boy named Jhon. He likked too play in da feelds and climb treas. Everyda he eated mangoes and dranks milk. His freinds where also happy with hym. They runned around and playeds footballs. One dae, they finds a big egg near the pond. It was shiney and glowed in sunlite. They dicided too keep it. Next mornning, the egg hatch and a tiny dragun camed out. It said “Helloo, I’m yur freind!” The boy’s smiled and they fel joy. Thus, the villag becamed famus for a magikal tail.\"\n"
      ],
      "metadata": {
        "id": "K5U28UqbYZ3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "gqNwL6invgNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "right_text=TextBlob(incorrect_text)\n",
        "right_text.correct().string"
      ],
      "metadata": {
        "id": "oAHGgLOnvnCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "w0PtsCMSv2kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "F6SxWH1fxJKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "ZXmkHlbgxQYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  new_text=[]\n",
        "  for word in text.split():\n",
        "    if word in stopwords.words('english'):\n",
        "      new_text.append('')\n",
        "    else:\n",
        "      new_text.append(word)\n",
        "  x=new_text[:]\n",
        "  new_text.clear()\n",
        "  return \" \".join(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uqfma9-4x-C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word_removal_text=\"The sun dipped below the horizon, painting the sky in fiery hues of orange, pink, and deep purple. A cool breeze swept across the meadow, rustling the tall grasses and carrying the scent of damp earth and blooming wildflowers. In the distance, the faint sound of crickets began to emerge, a gentle symphony announcing the arrival of twilight. The day's heat gave way to a peaceful stillness as the first star appeared, a tiny pinprick of light in the vast, darkening canvas above. It was a moment of quiet reflection, a perfect end to a long, busy day.\""
      ],
      "metadata": {
        "id": "7kl-6Bld0gr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(stop_word_removal_text)"
      ],
      "metadata": {
        "id": "JiBzWe4H002E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Emoji remover and Emoji meaning**"
      ],
      "metadata": {
        "id": "WAOWOCYh2ner"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "id": "n4dC2Nws06_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji"
      ],
      "metadata": {
        "id": "ys289nTc3Boj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emoji.demojize('python is ♨️'))"
      ],
      "metadata": {
        "id": "KpnX42dh3HxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "UBLIUhqp4beq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var='i am tamim. i am from varendra university.'"
      ],
      "metadata": {
        "id": "3YyW_UGO3Xu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var.split()"
      ],
      "metadata": {
        "id": "JpQrxpzp7t47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "var1=word_tokenize(var)\n",
        "var1"
      ],
      "metadata": {
        "id": "eUEDUECH6vQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentece tokenization\n",
        "var.split('.')"
      ],
      "metadata": {
        "id": "KkTrLdyj6zWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "pCB1GBci6-ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(var)"
      ],
      "metadata": {
        "id": "GtxrGose8T1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steaming**"
      ],
      "metadata": {
        "id": "orKZD1xp9gei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "XsXIz3vV8Z3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps=PorterStemmer()"
      ],
      "metadata": {
        "id": "tJXDjKtH9wLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_word(text):\n",
        "  return \" \".join([ps.stem(word) for word in text.split()])\n"
      ],
      "metadata": {
        "id": "jvqdn7Kr90Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample=\"walks walked walking walk\""
      ],
      "metadata": {
        "id": "TNGl7f3D-Km6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_word(sample)"
      ],
      "metadata": {
        "id": "zBKc10tY-Ugy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lematization**"
      ],
      "metadata": {
        "id": "0FOwIx3j_lNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "meXDKbMB-cfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "12OGLZ04AcFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "VWUU2apZEkaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the\"\n",
        "punctuations=\"?:,.\""
      ],
      "metadata": {
        "id": "l-_P4PDIEsDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_words=nltk.word_tokenize(sentence)\n",
        "sentence_words"
      ],
      "metadata": {
        "id": "jmI65ARRE0L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sentence_words:\n",
        "  if word in punctuations:\n",
        "    sentence_words.remove(word)"
      ],
      "metadata": {
        "id": "epxiFdjfFBsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"original words\")\n",
        "print(sentence_words)"
      ],
      "metadata": {
        "id": "PhlS-RhnFSNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n lematization words\")\n",
        "for word in sentence_words:\n",
        "  print(f\"{word: <20} {wordnet_lemmatizer.lemmatize(word, pos='v')}\")"
      ],
      "metadata": {
        "id": "lW_4OGliFaU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "AGI0L9jf6wXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install html\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "JXHS81af62eH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d018a78-bd27-4900-cb63-3228968a1884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/590.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "Collecting html\n",
            "  Downloading html-1.16.tar.gz (7.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import html\n",
        "import string\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# ---------- Remove Functions ----------\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')\n",
        "\n",
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([w for w in words if w.lower() not in stop_words])\n",
        "\n",
        "def stemming_text(text):\n",
        "    ps = PorterStemmer()\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([ps.stem(w) for w in words])\n",
        "\n",
        "def lemmatization_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(w) for w in words])\n",
        "\n",
        "# ---------- Handle Functions ----------\n",
        "\n",
        "def handle_emoji(text):\n",
        "    return emoji.demojize(text)  # ❤ → :red_heart:\n",
        "\n",
        "def handle_chat_words(text):\n",
        "    chat_dict = {\"u\": \"you\", \"ur\": \"your\", \"btw\": \"by the way\"}\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(chat_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: chat_dict[x.group()], text)\n",
        "\n",
        "def handle_urls(text):\n",
        "    return re.sub(r'(http\\S+)', 'URL_TOKEN', text)\n",
        "\n",
        "def handle_html_entities(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# ---------- POS Tagging ----------\n",
        "def get_pos_tags(text):\n",
        "    words = word_tokenize(text)\n",
        "    return pos_tag(words)\n",
        "\n",
        "# ---------- Pipeline: REMOVE ----------\n",
        "def preprocess_remove(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = lowercase_text(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatization_text(text)\n",
        "    return text\n",
        "\n",
        "# ---------- Pipeline: HANDLE ----------\n",
        "def preprocess_handle(text):\n",
        "    text = handle_html_entities(text)\n",
        "    text = handle_urls(text)\n",
        "    text = handle_chat_words(text)\n",
        "    text = handle_emoji(text)\n",
        "    text = lowercase_text(text)\n",
        "    return text\n",
        "\n",
        "# ---------- Example ----------\n",
        "text_data = 'I am tamim, i am from Varendra University and i ❤ my university, my univerty link is \"https://vu.edu.bd/\"'\n",
        "\n",
        "print(\"Remove Version:\\n\", preprocess_remove(text_data))\n",
        "print(\"\\nHandle Version:\\n\", preprocess_handle(text_data))\n",
        "print(\"\\nPOS Tags:\\n\", get_pos_tags(preprocess_handle(text_data)))\n",
        "\n"
      ],
      "metadata": {
        "id": "n72XrjKeGfaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910c6c44-aaf1-4d1c-b13f-7ee1ad59fae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remove Version:\n",
            " tamim varendra university university univerty link\n",
            "\n",
            "Handle Version:\n",
            " i am tamim, i am from varendra university and i :red_heart: my university, my univerty link is \"url_token\n",
            "\n",
            "POS Tags:\n",
            " [('i', 'NN'), ('am', 'VBP'), ('tamim', 'NN'), (',', ','), ('i', 'VBP'), ('am', 'VBP'), ('from', 'IN'), ('varendra', 'NN'), ('university', 'NN'), ('and', 'CC'), ('i', 'NN'), (':', ':'), ('red_heart', 'NN'), (':', ':'), ('my', 'PRP$'), ('university', 'NN'), (',', ','), ('my', 'PRP$'), ('univerty', 'JJ'), ('link', 'NN'), ('is', 'VBZ'), ('``', '``'), ('url_token', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.tree import Tree\n",
        "from nltk.grammar import CFG\n",
        "\n",
        "# Step 1: Tokenize and POS Tag\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "print(\"POS Tagging:\", tagged)\n",
        "\n",
        "# Step 2: Define a simple grammar\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> DT JJ JJ NN | DT JJ NN\n",
        "VP -> VBZ PP\n",
        "PP -> IN NP\n",
        "DT -> 'The' | 'the'\n",
        "JJ -> 'quick' | 'brown' | 'lazy'\n",
        "NN -> 'fox' | 'dog'\n",
        "VBZ -> 'jumps'\n",
        "IN -> 'over'\n",
        "\"\"\")\n",
        "\n",
        "# Step 3: Create a parser\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Step 4: Parse and show tree\n",
        "for tree in parser.parse(tokens):\n",
        "    tree.pretty_print()\n",
        "    tree.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvaWkA3K6019",
        "outputId": "37f0e5a0-7645-4da4-dd14-09ae2541b439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
            "                      S                        \n",
            "       _______________|_________                \n",
            "      |                         VP             \n",
            "      |                _________|___            \n",
            "      |               |             PP         \n",
            "      |               |     ________|___        \n",
            "      NP              |    |            NP     \n",
            "  ____|__________     |    |     _______|____   \n",
            " DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN\n",
            " |    |     |    |    |    |    |       |    |  \n",
            "The quick brown fox jumps over the     lazy dog\n",
            "\n",
            "                      S                        \n",
            "       _______________|_________                \n",
            "      |                         VP             \n",
            "      |                _________|___            \n",
            "      |               |             PP         \n",
            "      |               |     ________|___        \n",
            "      NP              |    |            NP     \n",
            "  ____|__________     |    |     _______|____   \n",
            " DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN\n",
            " |    |     |    |    |    |    |       |    |  \n",
            "The quick brown fox jumps over the     lazy dog\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8l_mjWzFqvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}